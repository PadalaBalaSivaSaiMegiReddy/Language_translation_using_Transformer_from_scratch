# Transformer from Scratch

This notebook builds a basic Encoder-Decoder variant of the Transformer architecture from scratch (Multi-Head Attention, Scaled Dot-Product Attention and Causal Masking included) in TensorFlow.

The Transformer is then tested on a simple seq2seq task: translating sentences from English to French.

## Prerequisites

- Python 3.6+
- TensorFlow 2.4+

## Getting Started

1. Clone this repository:

   ```bash
   git clone https://github.com/PadalaBalaSivaSaiMegiReddy/Language_translation_using_Transformer_from_scratch.git

2. Install the required packages:

   ```bash
    pip install -r requirements.txt

3. Run the Jupyter Notebook:

    ```bash
    jupyter notebook transformer.ipynb


## Data

The dataset used for this project is a small set of English and French sentence pairs, downloaded from the following link: http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip

## Acknowledgements

The implementation of the Transformer architecture in this project was based on the paper "Attention is All You Need" by Vaswani et al. (https://arxiv.org/abs/1706.03762).

The dataset used in this project is from the publicly available dataset "Annotated English-French Pairs" by Tatoeba Project (https://tatoeba.org/eng/downloads).

---

Contact me at [ LinkedIn ](https://www.linkedin.com/in/bala-siva-sai-megi-reddy-padala/) if you have any questions!